model:
  model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  new_model: "semikong-1.1b"
  use_4bit: True
  use_nested_quant: False
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  device_map: {"": 0}
  output_dir: "./results"
  dataset_name: "pentagoniac/SemiKong_Training_Datset"

training:
  num_train_epochs: 2
  per_device_train_batch_size: 3
  per_device_eval_batch_size: 3
  gradient_accumulation_steps: 1
  learning_rate: 2e-4
  max_grad_norm: 0.3
  weight_decay: 0.001
  max_seq_length: None
  fp16: False
  bf16: False
  packing: False
  gradient_checkpointing: True
  optim: "paged_adamw_32bit"
  lr_scheduler_type: "cosine"
  max_steps: -1
  warmup_ratio: 0.03
  group_by_length: True
  save_steps: 10
  logging_steps: 1

lora:
  lora_alpha: 16
  lora_dropout: 0.1
  lora_r: 64
